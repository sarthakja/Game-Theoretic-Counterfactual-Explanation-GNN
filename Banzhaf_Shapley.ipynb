{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kol6h58g-BUF"
      },
      "source": [
        "# Shapley Value Explainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Ko1R8z96BU"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqynIVa0-X_g"
      },
      "outputs": [],
      "source": [
        "# Make the `checkpoints` directory if it doesn't exist\n",
        "import pathlib\n",
        "import time\n",
        "pathlib.Path('/content/checkpoints').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Import the libs\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(\"Pytorch Version:\", torch.__version__)\n",
        "\n",
        "# Installing a few additional libs\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "\n",
        "# Install Pytorch Geometric\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "! pip install torch_geometric\n",
        "\n",
        "#Installing DGL\n",
        "!pip install dgl\n",
        "import numpy as np\n",
        "from torch_geometric.utils import subgraph, k_hop_subgraph, degree, mask_to_index\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from itertools import chain, combinations\n",
        "from random import sample\n",
        "from random import choice\n",
        "from random import randint\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHzw9E9W-Zgp"
      },
      "source": [
        "## Dataset setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM9iKWvW-fEE"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.datasets import BAShapes\n",
        "from torch_geometric.nn import GCN\n",
        "from torch_geometric.datasets import ExplainerDataset\n",
        "from torch_geometric.datasets.graph_generator import BAGraph\n",
        "import torch_geometric.transforms as T\n",
        "from dgl.data import TreeCycleDataset\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.utils import from_dgl\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "from dgl.data import TreeGridDataset\n",
        "\n",
        "#Load the BAShapes Dataset\n",
        "dataset = ExplainerDataset(\n",
        "    graph_generator=BAGraph(num_nodes=300, num_edges=5),\n",
        "    motif_generator='house',\n",
        "    num_motifs=80,\n",
        "    transform=T.Constant(),\n",
        ")\n",
        "data = dataset[0]\n",
        "\n",
        "############## Code block to Load TreeCycle and TreeGrid Datasets###########\n",
        "#Use the appropriate line from the 2 subsequent lines depending on whether TreeCycle or TreeGrid dataset is required\n",
        "# dataset = TreeCycleDataset()\n",
        "# #dataset = TreeGridDataset()\n",
        "\n",
        "# data = dataset[0]\n",
        "# data = from_dgl(data)\n",
        "\n",
        "# data['y'] = data['label']\n",
        "# data['x'] = data['feat']\n",
        "# del(data['feat'])\n",
        "# del(data['label'])\n",
        "# data = ToUndirected()(data)\n",
        "# print(data.is_directed())\n",
        "######################################################################\n",
        "\n",
        "# Printing out the dataset\n",
        "print(data)\n",
        "#print(data.x)\n",
        "print(data.edge_index)\n",
        "print((data.y)[200:300])\n",
        "print(\"***************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VAh47DrqPQW"
      },
      "outputs": [],
      "source": [
        "#This function is used to add random noise to the graph. Random noise means adding edges randomly to nodes that don't have any edge between them.\n",
        "\n",
        "#Parameter:\n",
        "# add_ratio: The fraction of existing count of edges to be added to the graph.\n",
        "def add_noise(add_ratio):\n",
        "  edgeIndexTransposed = torch.transpose(data.edge_index, 0 ,1)\n",
        "  edgeIndexNumpy = edgeIndexTransposed.numpy()\n",
        "  edgeIndexList = list(map(tuple, edgeIndexNumpy))\n",
        "  edgeIndexSet = set(edgeIndexList) #Set of tuples(each tuple is an edge)\n",
        "\n",
        "  #No of edges to add\n",
        "  edgesToAdd = int(add_ratio*len(edgeIndexSet))/2\n",
        "\n",
        "  edgesAdded = 0 #edges actually added\n",
        "  while(edgesAdded < edgesToAdd):\n",
        "    nodeList = sample(range((data.x.size())[0]),k = 2 )\n",
        "    nodeList = tuple(nodeList)\n",
        "    if(nodeList not in edgeIndexSet):\n",
        "      edgeIndexSet.add(nodeList)\n",
        "      reversedEdge = tuple(reversed(nodeList))\n",
        "      edgeIndexSet.add(reversedEdge)\n",
        "      edgesAdded+=1\n",
        "\n",
        "  edgeIndexArray = np.array(list(edgeIndexSet))\n",
        "  edgeIndexArray = np.transpose(edgeIndexArray)\n",
        "  edgeIndexTensor = torch.from_numpy(edgeIndexArray)\n",
        "  data.edge_index = edgeIndexTensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uncomment the lines below to call the add_noise function to add random noise"
      ],
      "metadata": {
        "id": "lW1BrG96BWoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZrQyyR4vU0S"
      },
      "outputs": [],
      "source": [
        "# noise_ratio = 0.1 # Adding 10% noise\n",
        "# add_noise(noise_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing out data after adding random noise. Uncomment the lines below to print data after adding random noise"
      ],
      "metadata": {
        "id": "pr0nEOAQBoK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K-b4yE5qOEk"
      },
      "outputs": [],
      "source": [
        "# print(data)\n",
        "# print(data.edge_index)\n",
        "# print((data.y)[200:300])\n",
        "# print(\"***************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5Z-Gx9Z-9v0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzZivS4b9wMk"
      },
      "source": [
        "# Saving and loading the noisy graph This cell stores the noisy graph in a pickle file, so that the same graph can be used across different runs of this notebook. Uncomment the lines below if you want to compare results by tweaking other parts of the model(such as the number of coalitions sampled for Banzhaf index) while keeping the graph constant. Make sure to replace the \"PATH_OF_FILE_THAT_WILL_HOLD_THE_NOISY_GRAPH\" with the appropriate path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1krfs9LmwOA"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# path = '/content/drive/MyDrive/'\n",
        "# file = open(path + <PATH_OF_FILE_THAT_WILL_HOLD_THE_NOISY_GRAPH>, 'wb')\n",
        "# pickle.dump(data, file)\n",
        "# file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uncomment the lines below to load the noisy graph. Make sure to replace the \"PATH_OF_FILE_THAT_HOLDS_THE_NOISY_GRAPH\" with the appropriate path."
      ],
      "metadata": {
        "id": "DyFTxGxsCPvW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4wO8yyB_9_7"
      },
      "outputs": [],
      "source": [
        "# # open the file, where you stored the noisy graph\n",
        "# path = '/content/drive/MyDrive/'\n",
        "# file = open(path + <PATH_OF_FILE_THAT_HOLDS_THE_NOISY_GRAPH>, 'rb')\n",
        "\n",
        "# # load information from that file\n",
        "# data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "rCnCN5OFR3w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9254WpFJunB"
      },
      "source": [
        "# Setup for edge mask. This is utilized for efficient calculation of Shapley and Banzhaf Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1NSwACrxePj"
      },
      "outputs": [],
      "source": [
        "edge_index_tp = torch.transpose(data.edge_index, 0 ,1)\n",
        "edge_index_np = edge_index_tp.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvoBS4UUSRGk"
      },
      "outputs": [],
      "source": [
        "ls = list(map(tuple, edge_index_np))\n",
        "tuple_edge_index = np.empty(len(ls), dtype=object)\n",
        "tuple_edge_index[:] = ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0EjsCAQ-feE"
      },
      "source": [
        "# Simple node classifier\n",
        "\n",
        "We are explaining the GCN model here, although our proposed method can be used to explain any model, it does not depend on the GNN. However, it is important that the model performs well on the node classification task for our explanation method to work well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYqDLfi_4oTG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.nn import GCNConv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqJbhWSQmyUZ"
      },
      "source": [
        "# Train and Test methods. The GCN model hyperparameters are different for BAShapes v/s TreeCycles and TreeGrid. See the cell below, and uncomment the appropriate lines according to whether BAShapes is being evaluated or TreeCycle/TreeGrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3TAi69zI1bO"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import Javascript  # Restrict height of output cell.\n",
        "# display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "data = data.to(\"cpu\")\n",
        "idx = torch.arange(data.num_nodes)\n",
        "train_idx, test_idx = train_test_split(idx, train_size=0.8, stratify=data.y)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#For BAShapes\n",
        "model = GCN(data.num_node_features, hidden_channels=20, num_layers=3,\n",
        "            out_channels=dataset.num_classes)\n",
        "\n",
        "#For TreeCycles and TreeGrid:\n",
        "# model = GCN(data.num_node_features, hidden_channels=128, num_layers=2,\n",
        "#             out_channels=dataset.num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.005)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.cross_entropy(out[train_idx], data.y[train_idx])\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
        "\n",
        "    train_correct = int((pred[train_idx] == data.y[train_idx]).sum())\n",
        "    train_acc = train_correct / train_idx.size(0)\n",
        "\n",
        "    test_correct = int((pred[test_idx] == data.y[test_idx]).sum())\n",
        "    test_acc = test_correct / test_idx.size(0)\n",
        "\n",
        "    return train_acc, test_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uncomment the lines below to train the GCN. This portion is kept commented to train the model only once, and then save the trained state for further use.\n",
        "\n"
      ],
      "metadata": {
        "id": "HW7QjPMCDRbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pbar = tqdm(range(1, 4001))\n",
        "# for epoch in pbar:\n",
        "#     loss = train()\n",
        "#     if epoch == 1 or epoch % 200 == 0:\n",
        "#         train_acc, test_acc = test()\n",
        "#         pbar.set_description(f'Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
        "#                              f'Test: {test_acc:.4f}')\n",
        "# pbar.close()\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "Idot8fjLDZRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.to(device)"
      ],
      "metadata": {
        "id": "J_lc43y4V8tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uncomment the lines below to save the trained model. You may want to save different versions of the model: model saved on noisy graph, and model saved on graph without noise. Make sure to replace \"PATH_TO_FILE_THAT_HOLDS_THE_SAVED_MODEL\" with the appropriate path"
      ],
      "metadata": {
        "id": "aEb3lxIyDgAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW8Z14QjKe1r"
      },
      "outputs": [],
      "source": [
        "# path = '/content/drive/MyDrive/'\n",
        "# file = open(path + <PATH_TO_FILE_THAT_HOLDS_THE_SAVED_MODEL>, 'wb')\n",
        "# torch.save(model, file)\n",
        "# file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the saved model. Make sure to replace the \"PATH_TO_FILE_THAT_HOLDS_THE_SAVED_MODEL\" with the appropriate path."
      ],
      "metadata": {
        "id": "IOLL4TBPDtvS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGEAgJCwyB2m"
      },
      "outputs": [],
      "source": [
        "# open a file, where you stored the saved model\n",
        "path = '/content/drive/MyDrive/'\n",
        "file = open(path + <PATH_TO_FILE_THAT_HOLDS_THE_SAVED_MODEL>, 'rb')\n",
        "\n",
        "\n",
        "# dump information to that file\n",
        "model = torch.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEa_k_9qHr55"
      },
      "source": [
        "## Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4vSobZF4tOS"
      },
      "outputs": [],
      "source": [
        "train_acc, test_acc = test()\n",
        "print(\"The train acc is: \", train_acc, \" and the test acc is: \", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIePn8tk-12l"
      },
      "source": [
        "# Explainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ioBOPSkuC65"
      },
      "source": [
        "## Some Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIoRJSpDqiyY"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import subgraph, k_hop_subgraph, degree, mask_to_index\n",
        "from torch_geometric.nn.conv import edge_conv\n",
        "\n",
        "#Returns hop_size_k hop subgraph around the node node_id\n",
        "def get_edges(data, node_id, hop_size_k):\n",
        "  #print(\"In get_edges, hop size is: \", hop_size_k)\n",
        "  return k_hop_subgraph(node_idx = node_id, num_hops = hop_size_k, edge_index = data.edge_index)\n",
        "\n",
        "#Returns class probabilities(a vector) and predicted class for node node_id.\n",
        "def get_node_class_prob (node_id, edge_ind):\n",
        "  model.eval()\n",
        "  edge_ind = edge_ind.to(device)\n",
        "  out = model(data.x, edge_ind)\n",
        "  pred = out.argmax(dim=-1)\n",
        "  probs = F.softmax(out, dim=-1).detach()\n",
        "  return probs[node_id], pred[node_id].item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4pdzID7uN50"
      },
      "outputs": [],
      "source": [
        "#Calculates the change in probability for node_id after removing the set of edges given by hard_set_of_edges\n",
        "def get_change_in_class_prob_for_set_of_edges (node_id, hard_set_of_edges):\n",
        "  # get original classification\n",
        "  model.eval()\n",
        "  original_prob_vector, original_class = get_node_class_prob(node_id, data.edge_index)\n",
        "  original_prob = original_prob_vector[original_class].item()\n",
        "  edge_index = data.edge_index\n",
        "\n",
        "  set_of_edges = list(i for i in hard_set_of_edges)\n",
        "  set_of_edges = set(set_of_edges)\n",
        "  def generate_mask(arr_ele):\n",
        "    return not(arr_ele in set_of_edges or tuple(reversed(arr_ele)) in set_of_edges)\n",
        "\n",
        "  # Generate mask\n",
        "  vectorized_mask_method = np.vectorize(generate_mask)\n",
        "  mask_arr = vectorized_mask_method(tuple_edge_index)\n",
        "\n",
        "  # Filter the edge_index_np(numpy array of the transposed edge_index) based on the mask generated above and convert it into a pytorch tensor\n",
        "  filtered_edge_index = edge_index_np[mask_arr]\n",
        "  filtered_edge_index = np.transpose(filtered_edge_index)\n",
        "  filtered_edge_index = torch.from_numpy(filtered_edge_index)\n",
        "\n",
        "  #remove edges and get new classification\n",
        "  #edge_index = remove_set_of_edges(edge_index, set_of_edges)\n",
        "  changed_prob_vector, changed_class = get_node_class_prob(node_id, filtered_edge_index)\n",
        "  changed_prob = changed_prob_vector[original_class].item()\n",
        "\n",
        "  #put back edges\n",
        "  #edge_index = add_set_of_edges(edge_index, set_of_edges)\n",
        "  return original_prob - changed_prob\n",
        "\n",
        "# Calculates if the predicted class changes for node_id after removing the set of edges given by hard_set_of_edges. If the class changes, then the return\n",
        "# value is 0, otherwise 1. hard_set_of_edges is a collection of edges where each edge is represented as a tuple.\n",
        "def get_change_in_class_for_set_of_edges (node_id, hard_set_of_edges):\n",
        "  # get original classification\n",
        "  model.eval()\n",
        "  original_prob_vector, original_class = get_node_class_prob(node_id, data.edge_index)\n",
        "  original_prob = original_prob_vector[original_class].item()\n",
        "  edge_index = data.edge_index\n",
        "\n",
        "  set_of_edges = list(i for i in hard_set_of_edges)\n",
        "  set_of_edges = set(set_of_edges)\n",
        "  def generate_mask(arr_ele):\n",
        "    return not(arr_ele in set_of_edges or tuple(reversed(arr_ele)) in set_of_edges)\n",
        "\n",
        "  # Generate mask\n",
        "  vectorized_mask_method = np.vectorize(generate_mask)\n",
        "  mask_arr = vectorized_mask_method(tuple_edge_index)\n",
        "\n",
        "  # Filter the edge_index_np(numpy array of the transposed edge_index) based on the mask generated above and convert it into a pytorch tensor\n",
        "  filtered_edge_index = edge_index_np[mask_arr]\n",
        "  filtered_edge_index = np.transpose(filtered_edge_index)\n",
        "  filtered_edge_index = torch.from_numpy(filtered_edge_index)\n",
        "\n",
        "  #remove edges and get new classification\n",
        "  #edge_index = remove_set_of_edges(edge_index, set_of_edges)\n",
        "  changed_prob_vector, changed_class = get_node_class_prob(node_id, filtered_edge_index)\n",
        "  changed_prob = changed_prob_vector[original_class].item()\n",
        "\n",
        "  #put back edges\n",
        "  #edge_index = add_set_of_edges(edge_index, set_of_edges)\n",
        "  return (original_prob - changed_prob, original_class == changed_class)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsO-mVrDu6r5"
      },
      "source": [
        "# Set the following hyperparameters in the cell below:\n",
        "\n",
        "**Budget**: The no of edges given out as the explanation\n",
        "\n",
        "**HOP_SIZE**: The no of hops up to which an edge is considered as a candidate for explanation  \n",
        "\n",
        "**Coalition_Count**: The no of coalitions to be considered when calculating the Banzhaf value.\n",
        "\n",
        "**THRESHOLD**: The threshold parameter in our paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_z7EbMgw_pA"
      },
      "outputs": [],
      "source": [
        "BUDGET = \"JUNK\" # No of explanation edges returned for each node\n",
        "HOP_SIZE = 1 # The no of hops upto which the edges for a node are considered\n",
        "Coalition_Count = 1500 # Parameter of banzhaf index: No of total coalitions considered from which the critical voters are considered\n",
        "THRESHOLD = \"JUNK\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to compute the Banzhaf Value. The implementation is based on the paper: [Data Banzhaf](https://proceedings.mlr.press/v206/wang23e.html)\n",
        "**Parameter:**\n",
        "\n",
        "node_id: The id of the node for which Banzhaf value of edges is calculated\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "A list of tuples, where each tuple consists of the edge and its Banzhaf Value"
      ],
      "metadata": {
        "id": "oXx_CT-wMRoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfhnxV2XPh13"
      },
      "outputs": [],
      "source": [
        "from itertools import chain, combinations\n",
        "from random import sample\n",
        "from random import choice\n",
        "from random import randint\n",
        "\n",
        "def get_explanation(node_id):\n",
        "  subset, edge_index, mapping, edge_mask = get_edges(data, node_id, HOP_SIZE)\n",
        "  #print(\"Edge index is: \", edge_index)\n",
        "  edge_list = torch.transpose(edge_index, 0, 1).tolist()\n",
        "  #print(edge_list)\n",
        "\n",
        "  for i in edge_list:\n",
        "    #print(i)\n",
        "    cur_from_edge = i[0]\n",
        "    cur_to_edge = i[1]\n",
        "    edge_list.remove([cur_to_edge, cur_from_edge])\n",
        "  #return edge_list\n",
        "\n",
        "  edge_set = set(tuple(x) for x in edge_list)\n",
        "\n",
        "  edgeUtilityDict = dict() # Dictionary containing the following key value pairs (edge: the sum of utilities of coalitions for which the edge is a part)\n",
        "  ret_set = set() #This stores the coalitions that have been sampled\n",
        "  counter = 0\n",
        "\n",
        "  #Initializing the edgeUtilityDict\n",
        "  for tup in edge_set:\n",
        "    edgeUtilityDict[tup] = (0,0)\n",
        "\n",
        "  #print(\"Coalition Count: \", Coalition_Count)\n",
        "  original_prob_vector, original_class = get_node_class_prob(node_id, data.edge_index)\n",
        "  original_prob = original_prob_vector[original_class].item()\n",
        "\n",
        "  coalition_dict=dict()#This stores the utility value of each sampled_coalition\n",
        "  while(len(ret_set) < Coalition_Count):\n",
        "    #print(\"In while\")\n",
        "    #sampled_coalition_list = sample(edge_set, k = randint(2,min(len(edge_set),BUDGET+2)) )\n",
        "    sampled_coalition_list = sample(edge_set,k = min(len(edge_set),BUDGET) )\n",
        "    sampled_coalition_list = tuple(sorted(sampled_coalition_list))\n",
        "    probab_change = get_change_in_class_prob_for_set_of_edges (node_id, sampled_coalition_list)\n",
        "\n",
        "    if((sampled_coalition_list in ret_set) or ((probab_change/original_prob)<THRESHOLD) ):\n",
        "      #print(\"if: \", sampled_coalition)\n",
        "      counter = counter + 1\n",
        "      if(counter==18):\n",
        "        break\n",
        "    else:\n",
        "      #print(\"else: \", sampled_coalition)\n",
        "      ret_set.add(sampled_coalition_list)\n",
        "      counter = 0\n",
        "      # probab_change = get_change_in_class_prob_for_set_of_edges (node_id, sampled_coalition_list)\n",
        "      coalition_dict[sampled_coalition_list] = probab_change\n",
        "      # totalUtility += probab_change\n",
        "\n",
        "  global BANZHAF_TIME\n",
        "  totalUtility = 0 #The sum of utilities of all the coalitions sampled so far\n",
        "  t00 = time.time()\n",
        "  for sampled_coalition_list in ret_set:\n",
        "    probab_change_fake = get_change_in_class_prob_for_set_of_edges (node_id, sampled_coalition_list)\n",
        "    probab_change = coalition_dict[sampled_coalition_list]\n",
        "    totalUtility += probab_change\n",
        "    for tup in sampled_coalition_list:\n",
        "      existingUtilitySum = edgeUtilityDict[tup][0]\n",
        "      existingCoalitionCount = edgeUtilityDict[tup][1]\n",
        "      edgeUtilityDict[tup] = (existingUtilitySum + probab_change, existingCoalitionCount+1)\n",
        "\n",
        "  banzhaf_dict = dict() # Dictionary storing the banzhaf value of each edge\n",
        "\n",
        "  for tup in edgeUtilityDict:\n",
        "    UtilitySum = edgeUtilityDict[tup][0]\n",
        "    CoalitionCount = edgeUtilityDict[tup][1]\n",
        "    if(CoalitionCount == 0 or CoalitionCount == len(ret_set)):\n",
        "      banzhaf_dict[tup] = 0\n",
        "    else:\n",
        "      RemainingUtility = totalUtility - UtilitySum\n",
        "      RemainingCoalitions = len(ret_set) - CoalitionCount\n",
        "      edgeBanzhafValue = (UtilitySum/CoalitionCount) - (RemainingUtility/RemainingCoalitions)\n",
        "      banzhaf_dict[tup] = edgeBanzhafValue\n",
        "\n",
        "  val_banzhaf = sorted(banzhaf_dict.keys(), key= lambda x: banzhaf_dict[x], reverse = True)\n",
        "  explanation_list_banzhaf = []\n",
        "  for edg in val_banzhaf:\n",
        "    explanation_list_banzhaf.append((edg,banzhaf_dict[edg]))\n",
        "  t11 = time.time()\n",
        "  BANZHAF_TIME+=(t11-t00)\n",
        "\n",
        "  return explanation_list_banzhaf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the SAMPLING_SIZE variable which holds the number of permutations considered when calculating the shapley value"
      ],
      "metadata": {
        "id": "oI3G33zsLd-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk3wOe9KBBPf"
      },
      "outputs": [],
      "source": [
        "SAMPLING_SIZE = 50 #Hyperparameter of shapley value: It is the no of permutations using which shapley value is calculated"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Compute the Shapley Value.\n",
        "Parameter:\n",
        "\n",
        "node_id: The id of the node for which Shapley value of edges is calculated\n",
        "\n",
        "Returns:\n",
        "\n",
        "A list of tuples, where each tuple consists of the edge and its Shapley Value"
      ],
      "metadata": {
        "id": "cMK5dVk_NY-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qugwrm72AkMt"
      },
      "outputs": [],
      "source": [
        "from itertools import chain, combinations\n",
        "from random import sample\n",
        "from random import choice\n",
        "from random import randint\n",
        "\n",
        "\n",
        "def get_explanation_shapley(node_id):\n",
        "  #print(\"hop size: \", hop_size)\n",
        "  subset, edge_index, mapping, edge_mask = get_edges(data, node_id, HOP_SIZE)\n",
        "  #print(\"Edge index is: \", edge_index)\n",
        "  edge_list = torch.transpose(edge_index, 0, 1).tolist()\n",
        "  #print(edge_list)\n",
        "\n",
        "  for i in edge_list:\n",
        "    #print(i)\n",
        "    cur_from_edge = i[0]\n",
        "    cur_to_edge = i[1]\n",
        "    edge_list.remove([cur_to_edge, cur_from_edge])\n",
        "  #return edge_list\n",
        "\n",
        "  shapely_dict = dict()\n",
        "  for edg in edge_list:\n",
        "    shapely_dict[tuple(edg)] = 0\n",
        "\n",
        "\n",
        "  rng = np.random.default_rng()\n",
        "  val_dictionary = dict()\n",
        "  for counter in range(0, SAMPLING_SIZE):\n",
        "    current_list = []\n",
        "    current_val = 0\n",
        "    perm = rng.permutation(edge_list)\n",
        "    perm = [tuple(x) for x in perm]\n",
        "    #t0 = time.time()\n",
        "    for ind in range(0, len(perm)):\n",
        "      current_list.append(perm[ind])\n",
        "      sorted_current_list = tuple(sorted(current_list))\n",
        "      # print(\"Single entity: \", perm[ind])\n",
        "      # print(\"First n entities:\", tuple(perm[0:ind+1]))\n",
        "      # print(type(tuple(perm[0:ind+1])))\n",
        "      if(sorted_current_list in val_dictionary):\n",
        "        val = val_dictionary[sorted_current_list]\n",
        "      else:\n",
        "        val = get_change_in_class_prob_for_set_of_edges(node_id, current_list)\n",
        "        val_dictionary[sorted_current_list] = val\n",
        "\n",
        "      #print(\"perm[ind]: \", tuple(perm[ind]))\n",
        "      shapely_dict[perm[ind]] += (val - current_val)\n",
        "      current_val = val\n",
        "\n",
        "    # t1 = time.time()\n",
        "    # print(\"Time taken: \",t1-t0)\n",
        "\n",
        "  for edg in shapely_dict:\n",
        "    shapely_dict[edg] /= SAMPLING_SIZE\n",
        "\n",
        "  val = sorted(shapely_dict.keys(), key= lambda x: shapely_dict[x], reverse = True)\n",
        "  explanation_list = []\n",
        "  for edg in val:\n",
        "    explanation_list.append((edg,shapely_dict[edg]))\n",
        "\n",
        "  return explanation_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uncomment the cell below to get an idea of the number of edges within the set hop size for each node in the graph"
      ],
      "metadata": {
        "id": "lAYnQCd4OIxw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQZoYm7f50JP"
      },
      "outputs": [],
      "source": [
        "# for node_id in range((data.x.size())[0]):\n",
        "#   subset, edge_index, mapping, edge_mask = get_edges(data, node_id, 1)\n",
        "#   print((edge_index.size()[1])/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling 50 % of nodes randomly from the graph. The sampling is done thrice. The cell is commented to perform the sampling only once and store the result later(see the next cell)"
      ],
      "metadata": {
        "id": "hTOSW_hiOa0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZfR3fI0m_Cl"
      },
      "outputs": [],
      "source": [
        "# numNodes = (data.x.size())[0]\n",
        "# testNodes = []\n",
        "# for i in range(0,3):\n",
        "#   testNodes.append(sample(range(0,numNodes), (int)(numNodes/2) ))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the sampled nodes for use in subsequent runs. Make sure to replace the \"PATH_TO_FILE_THAT_WILL_STORE_THE_SAMPLED_NODES\" with the appropriate path."
      ],
      "metadata": {
        "id": "ikcrKl2PO2oR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx020KI8VhQV"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# path = '/content/drive/MyDrive/'\n",
        "# file = open(path + <PATH_TO_FILE_THAT_WILL_STORE_THE_SAMPLED_NODES>, 'wb')\n",
        "# pickle.dump(testNodes, file)\n",
        "# file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the saved sampled nodes. Make sure to replace the \"PATH_TO_FILE_THAT_STORES_THE_SAMPLED_NODES\" with the appropriate path"
      ],
      "metadata": {
        "id": "xvxl5YVkQspx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBqdU8Yth6Fg"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# open a file, where you stored the pickled data\n",
        "path = '/content/drive/MyDrive/'\n",
        "file = open(path+ <PATH_TO_FILE_THAT_STORES_THE_SAMPLED_NODES>, 'rb')\n",
        "\n",
        "# dump information to that file\n",
        "testNodes = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FU_qP_8xQIv"
      },
      "outputs": [],
      "source": [
        "BANZHAF_TIME = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the following lists in the cell below:\n",
        "\n",
        "**thresLis**: The list of thresholds that will be evaluated for a given budget\n",
        "\n",
        "**budgetList**: The list of budgets that will be evaluated\n",
        "\n",
        "Note that the computation for the thresholds and budgets listed above will be done for both Shapley and Banzhaf.\n",
        "\n",
        "Make sure to replace the \"NAME_OF_FILE_THAT_STORES_THE_BANZHAF_AND_SHAPLEY_RESULTS\" with the appropriate path."
      ],
      "metadata": {
        "id": "r_DmuoooRtJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwAf9vo9x8M-"
      },
      "outputs": [],
      "source": [
        "thresLis = [0, 1e-2, 0.1]\n",
        "budgetList = [3,4,5]\n",
        "FILE_NAME = <NAME_OF_FILE_THAT_STORES_THE_BANZHAF_AND_SHAPLEY_RESULTS>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1tXnnXQSeF7"
      },
      "outputs": [],
      "source": [
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "702Jp8du_Mzd"
      },
      "source": [
        "# Average Fidelity for Banzhaf index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLyKy9-uv3Gs"
      },
      "outputs": [],
      "source": [
        "def banFun():\n",
        "  from statistics import mean\n",
        "\n",
        "  total_nodes = data.x.shape[0]\n",
        "  #print(\"Total nodes: \", len(testNodes[0]))\n",
        "  global BANZHAF_TIME\n",
        "  BANZHAF_TIME = 0\n",
        "  cumu_impact_banzhaf = []\n",
        "  # t0 = time.time()\n",
        "  for j in tqdm(testNodes):\n",
        "    run_impacts = []\n",
        "    for i in j:\n",
        "      subset, edge_index, mapping, edge_mask = get_edges(data, i, HOP_SIZE)\n",
        "      '''\n",
        "      if edge_index.size()[1] < 15:\n",
        "        print(i)\n",
        "        impact = get_impact_of_best_set_by_shapley_value(i, BUDGET, HOP_SIZE, SAMPLING_SIZE)\n",
        "        if impact != None: #Discard when explenations weren't generated\n",
        "          cumu_impact.append(impact)\n",
        "      '''\n",
        "      #print(i)\n",
        "      #impact = get_impact_of_best_set_by_shapley_value(i, BUDGET, HOP_SIZE, SAMPLING_SIZE)\n",
        "      explanationList = get_explanation(i)\n",
        "      t0 = time.time()\n",
        "      budgetExplanation = []\n",
        "      for jbud in range(0,min(BUDGET,len(explanationList)) ):\n",
        "        budgetExplanation.append(explanationList[jbud][0])\n",
        "\n",
        "      impact = get_change_in_class_for_set_of_edges (i, budgetExplanation)\n",
        "      # if impact > 0: #Discard when explenations weren't generated\n",
        "      #   cumu_impact.append(impact)\n",
        "      # else:\n",
        "      #   cumu_impact.append(0)\n",
        "      run_impacts.append(impact[1])\n",
        "      t1 = time.time()\n",
        "      BANZHAF_TIME+=t1-t0\n",
        "      # BANZHAF_TIME = BANZHAF_TIME/3.0\n",
        "\n",
        "    cumu_impact_banzhaf.append(mean(run_impacts))\n",
        "\n",
        "\n",
        "  # t1 = time.time()\n",
        "  BANZHAF_TIME = BANZHAF_TIME/3.0\n",
        "  opstr = \"Threshold = \"+str(THRESHOLD)+ \"  Budget= \"+ str(BUDGET) + \"--> Fidelity= \" + str(mean(cumu_impact_banzhaf)) + \" Variance= \" + str(np.var(cumu_impact_banzhaf)) + \" Std Dev = \" + str(np.std(cumu_impact_banzhaf))+ \" Total time taken: \" + str(BANZHAF_TIME)+\"\\n\"\n",
        "  path = '/content/drive/MyDrive/'\n",
        "  file = open(path+ FILE_NAME, 'a')\n",
        "  file.write(opstr)\n",
        "  file.close()\n",
        "  print(\"Threshold = \", THRESHOLD, \"  Budget= \", BUDGET,\"--> Fidelity= \",mean(cumu_impact_banzhaf),\" Variance= \", np.var(cumu_impact_banzhaf), \" Std Dev = \", str(np.std(cumu_impact_banzhaf)), \" Total time taken: \", BANZHAF_TIME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzuwCRj3BpTw"
      },
      "source": [
        "# Average Fidelity for Shapley Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk8Xqs440-A6"
      },
      "outputs": [],
      "source": [
        "def shapFun():\n",
        "  from statistics import mean\n",
        "  total_nodes = data.x.shape[0]\n",
        "  #print(\"Total nodes: \", len(testNodes))\n",
        "\n",
        "\n",
        "  cumu_impact = []\n",
        "  t0 = time.time()\n",
        "  for j in tqdm(testNodes):\n",
        "    #run_impacts = [[],[],[],[]]\n",
        "    run_impacts = []\n",
        "    for i in j:\n",
        "\n",
        "      subset, edge_index, mapping, edge_mask = get_edges(data, i, HOP_SIZE)\n",
        "      '''\n",
        "      if edge_index.size()[1] < 15:\n",
        "        print(i)\n",
        "        impact = get_impact_of_best_set_by_shapley_value(i, BUDGET, HOP_SIZE, SAMPLING_SIZE)\n",
        "        if impact != None: #Discard when explenations weren't generated\n",
        "          cumu_impact.append(impact)\n",
        "      '''\n",
        "      #print(i)\n",
        "      #impact = get_impact_of_best_set_by_shapley_value(i, BUDGET, HOP_SIZE, SAMPLING_SIZE)\n",
        "      explanationList = get_explanation_shapley(i)\n",
        "\n",
        "      #print(\"explanationList: \", explanationList)\n",
        "      # for bud in [3,5,6,7]:\n",
        "      #   budgetExplanation = []\n",
        "      #   for jbud in range(0,min(bud,len(explanationList)) ):\n",
        "      #     budgetExplanation.append(explanationList[jbud][0])\n",
        "\n",
        "      #   impact = get_change_in_class_for_set_of_edges (i, budgetExplanation)\n",
        "      #   if(bud==3):\n",
        "      #     run_impacts[0].append(impact[1])\n",
        "      #   else:\n",
        "      #     run_impacts[bud-4].append(impact[1])\n",
        "\n",
        "      budgetExplanation = []\n",
        "      for jbud in range(0,min(BUDGET,len(explanationList)) ):\n",
        "        budgetExplanation.append(explanationList[jbud][0])\n",
        "\n",
        "      impact = get_change_in_class_for_set_of_edges (i, budgetExplanation)\n",
        "      run_impacts.append(impact[1])\n",
        "\n",
        "    # mytup=()\n",
        "    # for myvar in range(0,len(run_impacts)):\n",
        "    #   mytup = mytup + (mean(run_impacts[myvar]),)\n",
        "\n",
        "    # cumu_impact.append(mytup)\n",
        "    cumu_impact.append(mean(run_impacts))\n",
        "\n",
        "  t1 = time.time()\n",
        "  ttaken = (t1-t0)/3.0\n",
        "  print(\"Total time taken: \", ttaken)\n",
        "\n",
        "  opstr = \"  Budget= \"+ str(BUDGET) + \"--> Fidelity= \" + str(mean(cumu_impact)) + \" Variance= \" + str(np.var(cumu_impact)) + \" Std Dev = \" + str(np.std(cumu_impact))+ \" Total time taken: \" + str(ttaken)+\"\\n\"\n",
        "  path = '/content/drive/MyDrive/'\n",
        "  file = open(path+ FILE_NAME, 'a')\n",
        "  file.write(opstr)\n",
        "  file.close()\n",
        "  print(\"Budget= \", BUDGET,\"--> Fidelity= \",mean(cumu_impact), \" Variance= \" + str(np.var(cumu_impact)), \" Std Dev = \", np.std(cumu_impact), \" Total time taken: \", ttaken)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The cell below runs a script to calculate average fidelity and time for different budget and threshold(For Banzhaf) combinations for Banzhaf and Shapley"
      ],
      "metadata": {
        "id": "GIplCl1pTarU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw6Sdo7BvjDc"
      },
      "outputs": [],
      "source": [
        "for scriptBudget in budgetList:\n",
        "  BUDGET = scriptBudget\n",
        "\n",
        "  path = '/content/drive/MyDrive/'\n",
        "  file = open(path+ FILE_NAME, 'a')\n",
        "  file.write(\"Banzhaf Results:\\n\")\n",
        "  file.close()\n",
        "\n",
        "  for scriptThres in thresLis:\n",
        "    THRESHOLD = scriptThres\n",
        "    banFun()\n",
        "\n",
        "  path = '/content/drive/MyDrive/'\n",
        "  file = open(path+ FILE_NAME, 'a')\n",
        "  file.write(\"\\nShapley Results:\\n\")\n",
        "  file.close()\n",
        "\n",
        "  shapFun()\n",
        "\n",
        "  path = '/content/drive/MyDrive/'\n",
        "  file = open(path+ FILE_NAME, 'a')\n",
        "  file.write(\"\\n\\n\")\n",
        "  file.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}